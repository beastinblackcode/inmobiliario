# Arquitectura del Sistema: Madrid Real Estate Tracker

## üìã √çndice

1. [Visi√≥n General](#visi√≥n-general)
2. [Componentes del Sistema](#componentes-del-sistema)
3. [Arquitectura de Datos](#arquitectura-de-datos)
4. [Flujo de Operaci√≥n](#flujo-de-operaci√≥n)
5. [Despliegue](#despliegue)
6. [Seguridad](#seguridad)
7. [Costes y Escalabilidad](#costes-y-escalabilidad)

---

## Visi√≥n General

### Prop√≥sito

Sistema de monitorizaci√≥n del mercado inmobiliario de Madrid que:
- Rastrea diariamente ~184 barrios de Madrid
- Detecta nuevas propiedades, cambios de precio y ventas
- Visualiza tendencias y m√©tricas del mercado
- Proporciona acceso web seguro a los datos

### Arquitectura General

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    USUARIO (Navegador)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ HTTPS
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              STREAMLIT CLOUD (Dashboard Web)                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  app.py (Streamlit Dashboard)                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Autenticaci√≥n multi-usuario                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Visualizaci√≥n de datos                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Filtros y an√°lisis                                ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                       ‚îÇ                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  database.py (Capa de Datos)                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Descarga DB desde Google Drive                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Consultas SQL                                     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   GOOGLE DRIVE      ‚îÇ
              ‚îÇ  real_estate.db     ‚îÇ
              ‚îÇ  (6 MB SQLite)      ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚ñ≤
                        ‚îÇ Upload manual
                        ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              M√ÅQUINA LOCAL (Scraping)                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  scraper.py                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Scraping de Idealista                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Detecci√≥n de cambios                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Actualizaci√≥n de DB local                        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                       ‚îÇ                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  database.py (Gesti√≥n DB)                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Inserciones/actualizaciones                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Marcado de vendidos                               ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                       ‚îÇ                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  real_estate.db (SQLite local)                       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   BRIGHT DATA       ‚îÇ
              ‚îÇ  (Web Unlocker)     ‚îÇ
              ‚îÇ  Proxy + Anti-bot   ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   IDEALISTA.COM     ‚îÇ
              ‚îÇ  (Fuente de datos)  ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Componentes del Sistema

### 1. Scraper (Ejecuci√≥n Local)

**Archivo:** `scraper.py`

**Responsabilidades:**
- Scraping de 184 barrios de Madrid
- Extracci√≥n de datos de propiedades
- Detecci√≥n de cambios (nuevas, actualizadas, vendidas)
- Actualizaci√≥n de base de datos local

**Tecnolog√≠as:**
- **Python 3.x**
- **BeautifulSoup4** - Parsing HTML
- **Requests** - HTTP requests
- **Bright Data Web Unlocker** - Proxy anti-bot
- **SQLite** - Base de datos local

**Datos Extra√≠dos:**
```python
{
    'listing_id': str,           # ID √∫nico de Idealista
    'title': str,                # T√≠tulo del anuncio
    'url': str,                  # URL completa
    'price': int,                # Precio en ‚Ç¨
    'distrito': str,             # Distrito de Madrid
    'barrio': str,               # Barrio espec√≠fico
    'rooms': int,                # N√∫mero de habitaciones
    'size_sqm': float,           # Superficie en m¬≤
    'floor': str,                # Planta
    'orientation': str,          # Interior/Exterior
    'seller_type': str,          # Particular/Agencia
    'is_new_development': bool,  # Obra nueva
    'description': str,          # Descripci√≥n parcial
}
```

**Frecuencia de Ejecuci√≥n:**
- Manual o programada (cron/scheduler)
- Recomendado: Diario

---

### 2. Base de Datos (SQLite)

**Archivo:** `real_estate.db`

**Esquema:**

```sql
CREATE TABLE listings (
    listing_id TEXT PRIMARY KEY,
    title TEXT,
    url TEXT,
    price INTEGER,
    distrito TEXT,
    barrio TEXT,
    rooms INTEGER,
    size_sqm REAL,
    floor TEXT,
    orientation TEXT,
    seller_type TEXT,
    is_new_development BOOLEAN,
    description TEXT,
    first_seen_date TEXT,      -- Fecha primera vez visto
    last_seen_date TEXT,       -- Fecha √∫ltima vez visto
    status TEXT DEFAULT 'active'  -- active/sold
);

-- √çndices para optimizar consultas
CREATE INDEX idx_status ON listings(status);
CREATE INDEX idx_distrito ON listings(distrito);
CREATE INDEX idx_last_seen ON listings(last_seen_date);
CREATE INDEX idx_price ON listings(price);
```

**Tama√±o:** ~6 MB (var√≠a seg√∫n n√∫mero de listings)

**Ubicaci√≥n:**
- **Local:** `/Users/luisnuno/Downloads/workspace/inmobiliario/real_estate.db`
- **Cloud:** Google Drive (compartido p√∫blicamente)

---

### 3. Dashboard Web (Streamlit Cloud)

**Archivo:** `app.py`

**Responsabilidades:**
- Interfaz web de visualizaci√≥n
- Autenticaci√≥n de usuarios
- An√°lisis y filtros de datos
- Exportaci√≥n de datos

**Caracter√≠sticas:**

#### Autenticaci√≥n
- Multi-usuario con contrase√±as individuales
- Sesi√≥n persistente
- Credenciales en Streamlit Secrets

#### Visualizaciones
- **M√©tricas principales:** Total activos, nuevos, vendidos, precio medio
- **Gr√°ficos:** Distribuci√≥n de precios, tendencias temporales
- **Tablas:** Listados detallados con filtros
- **Mapas:** Distribuci√≥n por distrito/barrio

#### Filtros
- Precio (m√≠n/m√°x)
- Distrito/Barrio
- Tipo de vendedor
- Estado (activo/vendido)
- Fecha

---

### 4. Capa de Datos (database.py)

**Archivo:** `database.py`

**Funciones Principales:**

```python
# Inicializaci√≥n
init_database()

# Descarga desde Google Drive (solo en cloud)
download_database_from_cloud()

# Operaciones CRUD
insert_listing(data: Dict) -> bool
update_listing(listing_id: str, data: Dict) -> bool
mark_as_sold(listing_ids: Set[str]) -> int

# Consultas
get_active_listing_ids() -> Set[str]
get_listings(status, distrito, barrio, ...) -> List[Dict]
get_price_statistics() -> Dict
get_sold_last_n_days(days: int) -> int
```

**Detecci√≥n de Entorno:**
```python
def is_streamlit_cloud():
    # Detecta si corre en Streamlit Cloud
    return "database" in st.secrets
```

---

## Arquitectura de Datos

### Flujo de Datos

```
1. SCRAPING (Local)
   ‚îú‚îÄ Idealista.com
   ‚îú‚îÄ Bright Data Proxy
   ‚îú‚îÄ BeautifulSoup parsing
   ‚îî‚îÄ SQLite local (real_estate.db)

2. UPLOAD (Manual)
   ‚îú‚îÄ Google Drive upload
   ‚îî‚îÄ Compartir p√∫blicamente

3. DASHBOARD (Cloud)
   ‚îú‚îÄ Download from Google Drive
   ‚îú‚îÄ Cache en Streamlit Cloud
   ‚îî‚îÄ Visualizaci√≥n web
```

### Sincronizaci√≥n de Datos

**Problema:** Base de datos local vs cloud

**Soluci√≥n Actual:** Upload manual a Google Drive

**Proceso:**
1. Ejecutar scraper localmente
2. Subir `real_estate.db` a Google Drive
3. Dashboard descarga autom√°ticamente en pr√≥ximo acceso

---

## Flujo de Operaci√≥n

### üîÑ Ciclo Completo de Actualizaci√≥n

#### Paso 1: Scraping Local

```bash
# En tu m√°quina local
cd /Users/luisnuno/Downloads/workspace/inmobiliario
source venv/bin/activate
python scraper.py
```

**Duraci√≥n:** ~2-4 horas (184 barrios)

**Output:**
- Base de datos actualizada: `real_estate.db`
- Logs de progreso
- Estad√≠sticas de cambios

**Cambios Detectados:**
- ‚úÖ **Nuevos listings:** Insertados con `first_seen_date = today`
- üîÑ **Actualizados:** `last_seen_date = today`, precio actualizado
- ‚ùå **Vendidos:** Marcados como `status = 'sold'`

---

#### Paso 2: Upload a Google Drive

**Opci√≥n A: Manual (Interfaz Web)**

1. Ve a [Google Drive](https://drive.google.com)
2. Busca el archivo `real_estate.db` existente
3. Click derecho ‚Üí "Gestionar versiones"
4. "Subir nueva versi√≥n"
5. Selecciona `/Users/luisnuno/Downloads/workspace/inmobiliario/real_estate.db`
6. Espera a que termine la subida

**Opci√≥n B: Manual (Drag & Drop)**

1. Ve a Google Drive
2. Borra el archivo `real_estate.db` antiguo
3. Arrastra el nuevo `real_estate.db` desde tu carpeta local
4. Aseg√∫rate que est√° compartido como "Cualquiera con el enlace puede ver"

**Opci√≥n C: Automatizada (gdrive CLI) - Opcional**

```bash
# Instalar gdrive (una sola vez)
brew install gdrive

# Autenticar (una sola vez)
gdrive about

# Subir archivo (reemplazar FILE_ID con tu ID)
gdrive update FILE_ID real_estate.db
```

**File ID actual:** `1ajdgLaneXwb6OWl_S727gwyYZUfrdF7p`

---

#### Paso 3: Actualizaci√≥n Autom√°tica del Dashboard

**Comportamiento:**
- Streamlit Cloud descarga la DB al iniciar
- Si ya existe, usa versi√≥n cacheada
- Para forzar actualizaci√≥n: **Reboot app** en Streamlit Cloud

**Verificaci√≥n:**
1. Abre el dashboard: `inmobiliario-beastinblackcode.streamlit.app`
2. Login con tus credenciales
3. Verifica la fecha de "√öltima actualizaci√≥n"
4. Comprueba las m√©tricas de nuevos/vendidos

---

### üìÖ Frecuencia Recomendada

| Actividad | Frecuencia | Duraci√≥n |
|-----------|-----------|----------|
| Scraping | Diario (noche) | 2-4h |
| Upload a Drive | Despu√©s de scraping | 2-5 min |
| Reboot dashboard | Opcional | 30s |

---

### üîß Troubleshooting

#### "Database file not found" en dashboard

**Causa:** DB no descargada desde Google Drive

**Soluci√≥n:**
1. Verifica que el file ID es correcto en secrets
2. Verifica que el archivo est√° compartido p√∫blicamente
3. Reboot app en Streamlit Cloud

#### Scraper muy lento

**Causa:** Bright Data rate limiting o problemas de red

**Soluci√≥n:**
1. Verifica credenciales de Bright Data
2. Reduce concurrencia (si aplicable)
3. Ejecuta en horarios de menos tr√°fico

#### Datos no actualizados en dashboard

**Causa:** Dashboard usando versi√≥n cacheada

**Soluci√≥n:**
1. Streamlit Cloud ‚Üí Settings ‚Üí Reboot app
2. Espera 30 segundos
3. Refresca navegador

---

## Despliegue

### Entorno Local (Scraping)

**Requisitos:**
- Python 3.8+
- pip
- virtualenv

**Setup:**
```bash
cd /Users/luisnuno/Downloads/workspace/inmobiliario
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

**Variables de Entorno (.env):**
```bash
BRIGHTDATA_USER=your_username
BRIGHTDATA_PASS=your_password
BRIGHTDATA_HOST=brd.superproxy.io:33335
```

---

### Streamlit Cloud (Dashboard)

**Configuraci√≥n:**

**Repository:** `github.com/beastinblackcode/inmobiliario`
**Branch:** `main`
**Main file:** `app.py`

**Secrets:**
```toml
[database]
google_drive_file_id = "1ajdgLaneXwb6OWl_S727gwyYZUfrdF7p"

[auth.users]
admin = "Contrase√±aAdmin123"
luis = "Contrase√±aLuis456"
```

**Deployment:**
- Auto-deploy en cada push a `main`
- Manual reboot disponible en Settings

---

## Seguridad

### Autenticaci√≥n

**M√©todo:** Username/Password con session state

**Almacenamiento:** Streamlit Secrets (encriptado)

**Caracter√≠sticas:**
- ‚úÖ Multi-usuario
- ‚úÖ Contrase√±as individuales
- ‚úÖ Sesi√≥n persistente
- ‚úÖ HTTPS autom√°tico

### Protecci√≥n de Datos

**Base de Datos:**
- ‚úÖ No contiene datos personales sensibles
- ‚úÖ Solo informaci√≥n p√∫blica de Idealista
- ‚úÖ Compartida p√∫blicamente (read-only)

**Credenciales:**
- ‚úÖ Bright Data en `.env` (no en git)
- ‚úÖ Streamlit secrets encriptados
- ‚úÖ `.gitignore` configurado

### Prevenci√≥n de Indexaci√≥n

**robots.txt:**
```
User-agent: *
Disallow: /
```

Bloquea crawlers de b√∫squeda.

---

## Costes y Escalabilidad

### Costes Actuales

**Bright Data:**
- ~$4 por 1000 requests
- ~5000 requests por scraping completo
- **Coste por scraping:** ~$20
- **Mensual (diario):** ~$600

**Streamlit Cloud:**
- **Gratis** (Community tier)
- L√≠mites: 1 app, recursos compartidos

**Google Drive:**
- **Gratis** (15 GB incluidos)
- DB actual: 6 MB

**Total mensual:** ~$600 (solo Bright Data)

---

### Optimizaciones Posibles

#### Reducir Costes de Scraping

1. **Scraping Selectivo:**
   - Solo barrios de inter√©s
   - Reducir frecuencia (semanal vs diario)

2. **Proxy Alternativo:**
   - Proxies residenciales m√°s baratos
   - Rotaci√≥n manual de IPs

3. **Rate Limiting:**
   - Delays entre requests
   - Menos p√°ginas por barrio

#### Escalabilidad

**Actual:** ~20,000 listings, 6 MB DB

**L√≠mites:**
- SQLite: Hasta ~140 TB (te√≥rico)
- Streamlit Cloud: ~1 GB RAM
- Google Drive: 15 GB gratis

**Proyecci√≥n:**
- 1 a√±o de datos: ~50 MB
- 5 a√±os: ~250 MB
- **Conclusi√≥n:** Escalable para a√±os

---

## Mejoras Futuras

### Automatizaci√≥n

**Opci√≥n 1: GitHub Actions**
```yaml
# .github/workflows/scrape.yml
name: Daily Scrape
on:
  schedule:
    - cron: '0 2 * * *'  # 2 AM diario
jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - run: python scraper.py
      - run: gdrive update $FILE_ID real_estate.db
```

**Opci√≥n 2: Cron Job Local**
```bash
# crontab -e
0 2 * * * cd /path/to/inmobiliario && ./run_scraper.sh
```

**Opci√≥n 3: Cloud Function**
- Google Cloud Functions
- AWS Lambda
- Ejecutar scraper en cloud

---

### Base de Datos Cloud

**Migrar a PostgreSQL/MySQL:**

**Ventajas:**
- ‚úÖ Actualizaci√≥n en tiempo real
- ‚úÖ No upload manual
- ‚úÖ Mejor concurrencia

**Desventajas:**
- ‚ùå Coste mensual ($10-50)
- ‚ùå M√°s complejidad

**Proveedores:**
- Supabase (PostgreSQL gratis hasta 500 MB)
- PlanetScale (MySQL gratis hasta 5 GB)
- Railway (PostgreSQL $5/mes)

---

### Notificaciones

**Alertas autom√°ticas:**
- Nuevas propiedades en barrios favoritos
- Bajadas de precio significativas
- Propiedades vendidas

**Canales:**
- Email (SendGrid, Mailgun)
- Telegram Bot
- Slack webhook

---

## Resumen Operativo

### ‚úÖ Checklist Diario

```
[ ] 1. Ejecutar scraper local (2-4h)
[ ] 2. Verificar logs de errores
[ ] 3. Subir real_estate.db a Google Drive (2 min)
[ ] 4. (Opcional) Reboot dashboard en Streamlit Cloud
[ ] 5. Verificar m√©tricas en dashboard
```

### üìä M√©tricas Clave

- **Listings activos:** ~20,000
- **Nuevos diarios:** ~200-500
- **Vendidos diarios:** ~100-300
- **Tiempo de scraping:** 2-4 horas
- **Tama√±o DB:** 6 MB
- **Coste mensual:** ~$600

---

## Contacto y Soporte

**Repositorio:** `github.com/beastinblackcode/inmobiliario`

**Dashboard:** `inmobiliario-beastinblackcode.streamlit.app`

**Documentaci√≥n:**
- `README.md` - Gu√≠a general
- `AUTH_SETUP.md` - Configuraci√≥n de autenticaci√≥n
- `MULTI_USER_AUTH.md` - Multi-usuario
- `walkthrough.md` - Implementaciones recientes
